import multiprocessing
from concurrent.futures import ProcessPoolExecutor
import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit
from deap import base, creator, tools, algorithms
import shap
from loguru import logger
import random
import ast
from sklearn.metrics import f1_score
import time
import matplotlib.pyplot as plt
import tempfile
import os
from functools import lru_cache
import ray
import sys
import ctypes

os.environ['OMP_NUM_THREADS'] = '1'  # é¿å…LightGBMçš„å¹¶è¡Œå†²çª
os.environ['WANDB_SILENT'] = 'true'  # å‡å°‘wandbçš„çº¿ç¨‹æ“ä½œ

logger.add("genetic_factor.debug.log", rotation="100 MB", level="DEBUG")  # ä¿å­˜æ‰€æœ‰æ—¥å¿—
logger.add("genetic_factor.info.log", rotation="100 MB", level="INFO")    # ä»…ä¿å­˜INFOåŠä»¥ä¸Š
logger.add(sys.stdout, level="INFO")  # æ§åˆ¶å°åªè¾“å‡ºINFOåŠä»¥ä¸Š
logger.add("genetic_factor.error.log", rotation="100 MB", level="ERROR")  # å•ç‹¬è®°å½•é”™è¯¯æ—¥å¿—

ray.init()

@ray.remote
def evaluate_remote(args):
    return evaluate_individual_wrapper(args)

def _parallel_map_wrapper(func, iterable):
    futures = [evaluate_remote.remote(arg) for arg in iterable]
    return ray.get(futures)

def evaluate_individual_wrapper(args):
    """ç‹¬ç«‹çš„è¯„ä¼°å‡½æ•°"""
    individual, data_dict, target_array, base_features, use_gpu = args
    try:
        logger.debug(f"å¼€å§‹è¯„ä¼°å› å­: {str(individual)[:50]}...")
        
        # å°†Individualå¯¹è±¡è½¬æ¢ä¸ºå…ƒç»„
        if hasattr(individual, '__iter__'):
            expr = tuple(individual)
        else:
            expr = individual
        
        # ä¿®æ”¹æ•°æ®å“ˆå¸Œæ–¹å¼
        hashable_data = []
        try:
            for k, v in data_dict.items():
                if isinstance(v, np.ndarray):
                    # ä½¿ç”¨æ•°ç»„çš„å†…å­˜è§†å›¾å’Œå±æ€§åˆ›å»ºå¯å“ˆå¸Œçš„å…ƒç»„
                    hashable_data.append((str(k), v.tobytes(), v.shape, v.dtype.str))
            
            # è½¬æ¢ä¸ºå…ƒç»„ç¡®ä¿å¯å“ˆå¸Œ
            data_tuple = tuple(sorted(hashable_data))
            
            # è¯„ä¼°å› å­
            factor_values = evaluate_expression(expr, data_tuple)
            
        except Exception as e:
            logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
            return (0, 0, 0, 0)
            
        if factor_values is None:
            return (0, 0, 0, 0)
            
        factor_values = np.asarray(factor_values, dtype=np.float64)
        
        # å¼‚å¸¸å€¼å¤„ç†
        if np.isnan(factor_values).any() or np.isinf(factor_values).any():
            logger.warning(f"âš ï¸ å› å­å€¼å¼‚å¸¸: åŒ…å«NaN/Inf - {str(individual)[:50]}...")
            return (0, 0, 0, 0)
            
        # å‘é‡åŒ–ä¿¡å·ç”Ÿæˆ
        lookback = 60  # ä½¿ç”¨æœ€è¿‘60åˆ†é’Ÿçš„æ»šåŠ¨çª—å£
        series = pd.Series(factor_values)
        rolling_high = series.rolling(lookback, min_periods=1).quantile(0.9)
        rolling_low = series.rolling(lookback, min_periods=1).quantile(0.1)
        
        # å‘é‡åŒ–ä¿¡å·ç”Ÿæˆ
        signal = np.select(
            [factor_values > rolling_high, factor_values < rolling_low],
            [1, -1],
            default=0
        )
        
        # å‘é‡åŒ–æ”¶ç›Šè®¡ç®— - ç§»é™¤æ‰‹ç»­è´¹è®¡ç®—
        returns = data_dict.get('return_1min', np.zeros_like(signal)) * signal
        net_returns = returns  # ç›´æ¥ä½¿ç”¨returnsï¼Œä¸å†å‡å»æ‰‹ç»­è´¹
        
        # ç®€åŒ–æŒ‡æ ‡è®¡ç®—
        total_return = np.exp(np.sum(np.log1p(net_returns))) - 1  # æ›´ç¨³å®šçš„ç´¯è®¡æ”¶ç›Šè®¡ç®—
        risk_free_rate = 0.02  # å¹´åŒ–2%
        excess_returns = net_returns - risk_free_rate/(252*24*60)
        valid_returns = net_returns[np.isfinite(net_returns)]  # è¿‡æ»¤æ— æ•ˆå€¼

        if len(valid_returns) < 2:  # æœ€å°‘éœ€è¦2ä¸ªæ ·æœ¬è®¡ç®—æ³¢åŠ¨ç‡
            sharpe_ratio = 0.0
        else:
            std_dev = np.std(valid_returns)
            if std_dev < 1e-8:
                std_dev = 1e-8
            sharpe_ratio = np.mean(excess_returns) / std_dev * np.sqrt(252*24*60)
        win_rate = np.mean(net_returns > 0)          # èƒœç‡
        max_drawdown = (np.maximum.accumulate(1 + net_returns) - (1 + net_returns)).max() # æœ€å¤§å›æ’¤

        # æ‰“å°å…³é”®æŒ‡æ ‡
        logger.info(
            f"\nğŸ“ˆ å› å­è¡¨ç° [{str(individual)[:30]}...]\n"
            f"â–¸ å¤æ™®æ¯”ç‡: {sharpe_ratio:.2f}\n"
            f"â–¸ ç´¯è®¡æ”¶ç›Š: {total_return:.2%}\n"
            f"â–¸ èƒœç‡: {win_rate:.2%}\n"
            f"â–¸ æœ€å¤§å›æ’¤: {max_drawdown:.2%}"
        )
        
        # åœ¨è¿”å›å‰æ·»åŠ æœ€ç»ˆæ¸…ç†
        factor_values = np.nan_to_num(factor_values, nan=0.0, posinf=0.0, neginf=0.0)
        factor_values = np.clip(factor_values, -1e10, 1e10)  # é˜²æ­¢æç«¯å€¼
        
        return (sharpe_ratio, total_return, win_rate, max_drawdown)
        
    except Exception as e:
        logger.error(f"å› å­è¯„ä¼°å¤±è´¥: {str(e)}")
        return (0, 0, 0, 0)

@lru_cache(maxsize=1000)
def evaluate_expression(expr, data_dict_hash):
    """å¢å¼ºå‹è¡¨è¾¾å¼è¯„ä¼°å‡½æ•°"""
    try:
        # é‡å»ºæ•°æ®å­—å…¸
        data_dict = {}
        if isinstance(data_dict_hash, (list, tuple)):  # æ·»åŠ ç±»å‹æ£€æŸ¥
            for k, data_bytes, shape, dtype_str in data_dict_hash:
                # ä»å­—èŠ‚é‡å»ºæ•°ç»„
                data = np.frombuffer(data_bytes, dtype=np.dtype(dtype_str)).reshape(shape)
                data_dict[k] = data
        else:
            logger.error(f"æ— æ•ˆçš„æ•°æ®å­—å…¸å“ˆå¸Œç±»å‹: {type(data_dict_hash)}")
            return None
            
        # å¤„ç†åŸºæœ¬ç‰¹å¾ç›´æ¥è¿”å›
        if isinstance(expr, str):
            if expr in data_dict:
                return data_dict[expr]
            logger.warning(f"ç‰¹å¾å {expr} ä¸åœ¨æ•°æ®å­—å…¸ä¸­")
            return None
            
        # ç¡®ä¿expræ˜¯å…ƒç»„
        if not isinstance(expr, (list, tuple)):
            logger.warning(f"æ— æ•ˆçš„è¡¨è¾¾å¼ç±»å‹: {type(expr)}")
            return None
            
        # å°†listè½¬æ¢ä¸ºtupleä»¥ç¡®ä¿å¯å“ˆå¸Œ
        if isinstance(expr, list):
            expr = tuple(expr)
            
        op = expr[0]
        
        # å¤„ç†æ»šåŠ¨çª—å£æ“ä½œï¼ˆå¢åŠ å‚æ•°éªŒè¯å’ŒåµŒå¥—å¤„ç†ï¼‰
        if op == 'rolling':
            if len(expr) != 4:
                return None
            _, operation, feature_expr, window = expr
            
            # å…ˆè¯„ä¼°ç‰¹å¾è¡¨è¾¾å¼
            feature_values = evaluate_expression(feature_expr, data_dict_hash)
            if feature_values is None:
                return None
                
            # éªŒè¯windowå‚æ•°
            if isinstance(window, tuple):  # å¦‚æœwindowæ˜¯è¡¨è¾¾å¼
                window_value = evaluate_expression(window, data_dict_hash)
                if window_value is None:
                    return None
                # å–windowè¡¨è¾¾å¼ç»“æœçš„å‡å€¼ä½œä¸ºçª—å£å¤§å°
                window = int(np.mean(window_value))
            
            # ç¡®ä¿windowæ˜¯æœ‰æ•ˆçš„æ­£æ•´æ•°
            try:
                window = int(window)
                if window <= 0:
                    logger.warning(f"æ— æ•ˆçš„çª—å£å¤§å°: {window}ï¼Œä½¿ç”¨é»˜è®¤å€¼5")
                    window = 5
            except (TypeError, ValueError):
                logger.warning(f"æ— æ•ˆçš„çª—å£å‚æ•°ç±»å‹: {type(window)}ï¼Œä½¿ç”¨é»˜è®¤å€¼5")
                window = 5
            
            # éªŒè¯operationå‚æ•°
            valid_operations = ['mean', 'std', 'max', 'min']
            if operation not in valid_operations:
                logger.warning(f"æ— æ•ˆçš„æ»šåŠ¨æ“ä½œ: {operation}ï¼Œä½¿ç”¨mean")
                operation = 'mean'
            
            try:
                # ä½¿ç”¨pandasçš„rollingæ“ä½œ
                series = pd.Series(feature_values)
                result = series.rolling(window, min_periods=1).agg(operation)
                # å¡«å……å¼€å§‹çš„NaNå€¼
                result = result.fillna(method='bfill').fillna(method='ffill')
                return result.values
            except Exception as e:
                logger.error(f"æ»šåŠ¨è®¡ç®—å¤±è´¥: {operation} window={window}, é”™è¯¯: {str(e)}")
                return None

        # å¤„ç†äºŒå…ƒè¿ç®—ï¼ˆå¢åŠ ç©ºå€¼æ£€æŸ¥å’Œé•¿åº¦å¯¹é½ï¼‰
        elif op in ['add', 'sub', 'mul', 'div', 'corr', 'cov', 'ratio', 'residual']:
            if len(expr) != 3:
                return None
            a = evaluate_expression(expr[1], data_dict_hash)
            b = evaluate_expression(expr[2], data_dict_hash)
            
            # æ–°å¢ç©ºå€¼æ£€æŸ¥
            if a is None or b is None:
                return None
                
            # ç»Ÿä¸€æ•°ç»„é•¿åº¦ï¼ˆå–æœ€å°å€¼ï¼‰
            min_length = min(len(a), len(b))
            a = a[:min_length]
            b = b[:min_length]
            
            # æ‰§è¡Œè¿ç®—
            if op == 'add':
                return a + b
            elif op == 'sub':
                return a - b
            elif op == 'mul':
                return a * b
            elif op == 'div':
                return np.divide(a, b, out=np.zeros_like(a), where=b!=0)
            # ... [å…¶ä»–äºŒå…ƒè¿ç®—å¤„ç†ä¿æŒä¸å˜]

        # å¤„ç†ä¸€å…ƒè¿ç®—ï¼ˆå¢åŠ ç©ºå€¼æ£€æŸ¥å’Œé•¿åº¦å¤„ç†ï¼‰
        elif op in ['sqrt', 'log', 'zscore', 'delta', 'lag']:
            if len(expr) != 2:
                return None
            x = evaluate_expression(expr[1], data_dict_hash)
            
            # æ–°å¢ç©ºå€¼æ£€æŸ¥
            if x is None:
                return None
                
            x_length = len(x)
            if x_length == 0:
                return None
                
            try:
                if op == 'sqrt':
                    return np.sqrt(np.abs(x))  # é˜²æ­¢è´Ÿå€¼
                elif op == 'log':
                    return np.log(np.abs(x) + 1e-8)  # é˜²æ­¢é›¶å€¼
                elif op == 'zscore':
                    return (x - np.mean(x)) / (np.std(x) + 1e-8)
                elif op == 'delta':
                    return np.diff(x, prepend=x[0])
                elif op == 'lag':
                    return np.roll(x, shift=1)  # æ”¹ä¸ºå‘é‡åŒ–æ“ä½œ
            except Exception as e:
                logger.error(f"{op}è¿ç®—å¤±è´¥: {str(e)}")
                return None

    except Exception as e:
        logger.error(f"è¡¨è¾¾å¼æ‰§è¡Œå¤±è´¥: {expr}, é”™è¯¯: {str(e)}")
        return None

def visualize_expression(expr, level=0):
    """å¯è§†åŒ–è¡¨è¾¾å¼æ ‘ç»“æ„"""
    if isinstance(expr, str):
        return "  " * level + expr + "\n"
    if isinstance(expr, tuple):
        result = "  " * level + expr[0] + "\n"
        for e in expr[1:]:
            result += visualize_expression(e, level+1)
        return result
    return "  " * level + "Invalid Node\n"

class GeneticFactorMiner:
    def __init__(self, data, target, population_size=50, generations=10, 
                 cx_prob=0.7, mut_prob=0.3, elite_size=10, 
                 base_features=['price_momentum', 'volatility_ratio'],
                 use_gpu=True):
        """
        åˆå§‹åŒ–é—ä¼ ç®—æ³•å‚æ•°
        :param data: è¾“å…¥æ•°æ® (DataFrame)
        :param target: ç›®æ ‡å˜é‡ (Series)
        :param population_size: ç§ç¾¤å¤§å°
        :param generations: è¿­ä»£ä»£æ•°
        :param cx_prob: äº¤å‰æ¦‚ç‡
        :param mut_prob: å˜å¼‚æ¦‚ç‡
        :param elite_size: ç²¾è‹±ä¿ç•™æ•°é‡
        :param base_features: åŸºç¡€ç‰¹å¾åˆ—è¡¨
        :param use_gpu: æ˜¯å¦ä½¿ç”¨GPU
        """
        self.data = data
        self.target = target
        self.pop_size = population_size
        self.generations = generations
        self.cx_prob = cx_prob
        self.mut_prob = mut_prob
        self.elite_size = elite_size
        
        # å®šä¹‰é—ä¼ ç®—æ³•åŸºæœ¬å…ƒç´ 
        creator.create("FitnessMax", base.Fitness, weights=(1.0, 1.0, 1.0, 1.0))
        creator.create("Individual", list, fitness=creator.FitnessMax)
        
        self.toolbox = base.Toolbox()
        self._setup_genetic_operators()
        
        # SHAPè§£é‡Šå™¨åˆå§‹åŒ–
        self.explainer = None
        self.shap_values = None
        
        self.use_gpu = use_gpu
        self.n_jobs = multiprocessing.cpu_count()  # è‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°
        
        # å°†æ•°æ®è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œé¿å…DataFrameåºåˆ—åŒ–é—®é¢˜
        self.data_array = {col: data[col].values for col in data.columns}
        self.target_array = target.values
        self.column_names = data.columns.tolist()
        
        # æ·»åŠ è¿™ä¸€è¡Œæ¥ä¿å­˜base_featureså‚æ•°
        self.base_features = base_features  # æ–°å¢è¡Œ
        
        # éªŒè¯æ•°æ®åŒ…å«æ‰€æœ‰åŸºç¡€ç‰¹å¾
        missing = [f for f in self.base_features if f not in data.columns]
        if missing:
            raise ValueError(f"æ•°æ®ç¼ºå¤±å¿…éœ€çš„åŸºç¡€ç‰¹å¾: {missing}")
        
        # éªŒè¯æ•°æ®æ ¼å¼
        for col, values in self.data_array.items():
            if not isinstance(values, np.ndarray):
                raise TypeError(f"åˆ— {col} çš„æ•°æ®ç±»å‹åº”ä¸ºnumpyæ•°ç»„ï¼Œå®é™…ä¸º {type(values)}")
            if len(values) != len(target):
                raise ValueError(f"åˆ— {col} çš„é•¿åº¦ä¸ç›®æ ‡å˜é‡ä¸åŒ¹é…")
    
    def _setup_genetic_operators(self):
        """é…ç½®é—ä¼ ç®—æ³•æ“ä½œç¬¦"""
        # ä¿®æ”¹ä¸ªä½“ç”Ÿæˆæ–¹å¼ï¼Œç¡®ä¿ç›´æ¥è¿”å›å…ƒç»„
        self.toolbox.register("expr", self._generate_random_expression, min_depth=2, max_depth=4)
        # ä¸å†å°†è¡¨è¾¾å¼åŒ…è£…ä¸ºå…ƒç»„ï¼Œå› ä¸º_generate_random_expressionå·²ç»è¿”å›å…ƒç»„
        self.toolbox.register("individual", tools.initIterate, creator.Individual, self.toolbox.expr)
        self.toolbox.register("population", tools.initRepeat, list, self.toolbox.individual)
        
        # æ³¨å†Œé—ä¼ æ“ä½œ
        self.toolbox.register("evaluate", self._evaluate_individual)
        self.toolbox.register("mate", self._cx_expression)
        self.toolbox.register("mutate", self._mut_expression)
        self.toolbox.register("select", tools.selTournament, tournsize=3)
    
    def _build_expr(self, min_depth, max_depth, depth=0):
        """æ„å»ºéšæœºå› å­è¡¨è¾¾å¼ï¼ˆç¡®ä¿è¿”å›å…ƒç»„ï¼Œé™åˆ¶æœ€å¤§åµŒå¥—æ·±åº¦ä¸º7ï¼‰"""
        try:
            # å½“è¾¾åˆ°æœ€å¤§æ·±åº¦æˆ–è¶…è¿‡7å±‚æ—¶ç›´æ¥è¿”å›ç‰¹å¾å­—ç¬¦ä¸²
            if depth >= max_depth or depth >= 7:  # æ·»åŠ æ·±åº¦é™åˆ¶
                return random.choice(self.base_features)
            
            # æ ¹æ®å½“å‰æ·±åº¦è°ƒæ•´æ“ä½œç¬¦çš„é€‰æ‹©æ¦‚ç‡
            if depth >= 5:  # å½“æ·±åº¦è¾ƒå¤§æ—¶ï¼Œå¢åŠ è¿”å›åŸºç¡€ç‰¹å¾çš„æ¦‚ç‡
                if random.random() < 0.6:  # 60%æ¦‚ç‡ç›´æ¥è¿”å›åŸºç¡€ç‰¹å¾
                    return random.choice(self.base_features)
            
            op = random.choice([
                'add', 'sub', 'mul', 'div', 'sqrt', 'log',
                'zscore', 'delta', 'lag', 'corr', 'cov',
                'ratio', 'residual', 'rolling'
            ])
            
            # ç¡®ä¿æ“ä½œç¬¦ç»“æ„æ­£ç¡®å¹¶è¿”å›å…ƒç»„
            if op == 'rolling':
                return tuple([op, 
                        random.choice(['mean', 'std', 'max', 'min']),
                        random.choice(self.base_features),  # rollingæ“ä½œç›´æ¥ä½¿ç”¨åŸºç¡€ç‰¹å¾
                        random.randint(5, 60)])
            elif op in ['add', 'sub', 'mul', 'div', 'corr', 'cov', 'ratio', 'residual']:
                return tuple([op, 
                        self._build_expr(min_depth, max_depth, depth+1),
                        self._build_expr(min_depth, max_depth, depth+1)])
            elif op in ['sqrt', 'log', 'zscore', 'delta', 'lag']:
                return tuple([op, 
                        self._build_expr(min_depth, max_depth, depth+1)])
            else:
                return random.choice(self.base_features)
            
        except Exception as e:
            logger.error(f"æ„å»ºè¡¨è¾¾å¼å¤±è´¥: {str(e)}")
            return random.choice(self.base_features)

    def _generate_random_expression(self, min_depth=2, max_depth=3):
        """ç”Ÿæˆéšæœºå› å­è¡¨è¾¾å¼ï¼ˆç¡®ä¿ä¸è¶…è¿‡7å±‚ï¼‰"""
        attempt = 0
        max_attempts = 5
        max_depth = min(max_depth, 7)  # ç¡®ä¿max_depthä¸è¶…è¿‡7
        
        while attempt < max_attempts:
            attempt += 1
            try:
                expr = self._build_expr(min_depth, max_depth)
                if self._validate_expression(expr) and self._get_depth(expr) <= 7:
                    return expr
                elif attempt == max_attempts:
                    logger.warning("ç”Ÿæˆçš„è¡¨è¾¾å¼æ·±åº¦è¶…è¿‡7å±‚ï¼Œè¿”å›ç®€å•è¡¨è¾¾å¼")
                    return random.choice(self.base_features)
            except RecursionError:
                logger.warning("é€’å½’æ·±åº¦è¿‡å¤§ï¼Œå°è¯•ç”Ÿæˆæ›´ç®€å•è¡¨è¾¾å¼")
                return random.choice(self.base_features)
            except Exception as e:
                logger.warning(f"ç”Ÿæˆè¡¨è¾¾å¼å‡ºé”™: {str(e)}")
        
        base = random.choice(self.base_features)
        logger.info(f"è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°ï¼Œè¿”å›åŸºç¡€ç‰¹å¾: {base}")
        return base

    def _validate_expression(self, expr):
        """éªŒè¯è¡¨è¾¾å¼ç»“æ„ï¼ˆé˜²æ­¢ç‰¹å¾åè¢«æ‹†åˆ†ï¼‰"""
        if isinstance(expr, str):
            return expr in self.base_features
        
        if not isinstance(expr, tuple):
            return False
        
        # æ£€æŸ¥æ“ä½œç¬¦ç»“æ„
        if expr[0] == 'rolling':
            return (len(expr) == 4 and 
                   expr[1] in ['mean', 'std', 'max', 'min'] and
                   isinstance(expr[2], str) and  # ç‰¹å¾åå¿…é¡»æ˜¯å­—ç¬¦ä¸²
                   expr[2] in self.base_features)
        elif expr[0] in ['add', 'sub', 'mul', 'div', 'corr', 'cov', 'ratio', 'residual']:
            return len(expr) == 3 and all(self._validate_expression(e) for e in expr[1:])
        elif expr[0] in ['sqrt', 'log', 'zscore', 'delta', 'lag']:
            return len(expr) == 2 and self._validate_expression(expr[1])
        return False

    def _get_depth(self, expr):
        """è®¡ç®—è¡¨è¾¾å¼æ·±åº¦"""
        if isinstance(expr, str):
            return 1
        return 1 + max(self._get_depth(e) for e in expr[1:])
    
    def _evaluate_individual(self, individual):
        """è¯„ä¼°å‰å¢åŠ ç»“æ„æ ¡éªŒ"""
        # å°†Individualå¯¹è±¡è½¬æ¢ä¸ºæ™®é€šå…ƒç»„
        if hasattr(individual, '__iter__'):
            expr = tuple(individual)
        else:
            expr = individual
        
        if not self._validate_expression(expr):
            logger.warning(f"éæ³•ä¸ªä½“ç»“æ„: {expr}")
            return (0, 0, 0, 0)
        return evaluate_individual_wrapper((
            expr,  # ä¼ é€’è½¬æ¢åçš„å…ƒç»„
            self.data_array,
            self.target_array,
            self.base_features,
            self.use_gpu
        ))
    
    def _cx_expression(self, ind1, ind2):
        """å¢å¼ºå‹äº¤å‰æ“ä½œ"""
        try:
            # å¢åŠ ç±»å‹æ£€æŸ¥
            if not (isinstance(ind1, (list, tuple)) and isinstance(ind2, (list, tuple))):
                return ind1, ind2
            
            # é™åˆ¶äº¤å‰æ·±åº¦
            max_depth = 3
            if self._get_depth(ind1) > max_depth or self._get_depth(ind2) > max_depth:
                return ind1, ind2
            
            # å¯»æ‰¾æœ‰æ•ˆäº¤å‰ç‚¹
            cx_points = []
            for i, e in enumerate(ind1):
                if isinstance(e, (list, tuple)) and len(e) > 1:
                    cx_points.append(i)
            if not cx_points:
                return ind1, ind2
            
            # æ‰§è¡Œäº¤å‰
            cx_point = random.choice(cx_points)
            ind1[cx_point], ind2[cx_point] = ind2[cx_point], ind1[cx_point]
            
            # äº¤å‰åéªŒè¯
            if not self._validate_expression(ind1):
                ind1[:] = self._generate_random_expression()
            if not self._validate_expression(ind2):
                ind2[:] = self._generate_random_expression()
            
            return ind1, ind2
        except Exception as e:
            logger.warning(f"äº¤å‰æ“ä½œå¤±è´¥: {str(e)}")
            return ind1, ind2
    
    def _mut_expression(self, individual):
        """å˜å¼‚æ“ä½œï¼šå¢åŠ ç»“æ„æ ¡éªŒ"""
        try:
            if not isinstance(individual, (list, tuple)):
                return individual,
            
            index = random.randrange(len(individual))
            original_subtree = individual[index]
            
            # ç”Ÿæˆæ–°å­æ ‘ç›´åˆ°åˆæ³•
            for _ in range(5):
                new_subtree = self._generate_random_expression(min_depth=1, max_depth=2)
                if self._validate_expression(new_subtree):
                    individual[index] = new_subtree
                    del individual.fitness.values
                    logger.debug(f"æˆåŠŸæ›¿æ¢å­æ ‘: {original_subtree} -> {new_subtree}")
                    return individual,
            
            logger.warning("æ— æ³•ç”Ÿæˆåˆæ³•å­æ ‘ï¼Œä¿æŒåŸçŠ¶")
            return individual,
        except Exception as e:
            logger.warning(f"å˜å¼‚æ“ä½œå¤±è´¥: {str(e)}")
            return individual,
    
    def _shap_analysis(self, best_factors):
        """æ‰§è¡ŒSHAPåˆ†æ"""
        # ç”Ÿæˆæœ€ä½³å› å­é›†
        X = pd.DataFrame()
        for i, factor in enumerate(best_factors):
            X[f'factor_{i}'] = evaluate_expression(factor, self.data_array)
        
        # è°ƒæ•´æ¨¡å‹å‚æ•°
        model = lgb.LGBMClassifier(
            n_estimators=100,          # å‡å°‘æ ‘çš„æ•°é‡
            num_leaves=31,             # å‡å°‘å¶å­èŠ‚ç‚¹æ•°é‡
            max_depth=5,               # æ·»åŠ æœ€å¤§æ·±åº¦é™åˆ¶
            min_child_samples=50,      # å¢åŠ å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
            learning_rate=0.1,         # æé«˜å­¦ä¹ ç‡
            reg_alpha=0.1,             # æ·»åŠ L1æ­£åˆ™åŒ–
            reg_lambda=0.1,            # æ·»åŠ L2æ­£åˆ™åŒ–
            class_weight='balanced',   # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
            device='gpu' if self.use_gpu else 'cpu',
            n_jobs=self.n_jobs
        )
        
        # æ·»åŠ æ•°æ®å¹³è¡¡æ ¡éªŒ
        logger.info(f"ç›®æ ‡å˜é‡åˆ†å¸ƒ:\n{self.target.value_counts(normalize=True)}")
        
        # ä½¿ç”¨äº¤å‰éªŒè¯
        tscv = TimeSeriesSplit(n_splits=3)
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = self.target.iloc[train_idx], self.target.iloc[val_idx]
            
            model.fit(X_train, y_train,
                      eval_set=[(X_val, y_val)],
                      early_stopping_rounds=20,
                      verbose=10)
        
        # è®¡ç®—SHAPå€¼
        self.explainer = shap.TreeExplainer(model)
        self.shap_values = self.explainer.shap_values(X)
        
        # å¯è§†åŒ–åˆ†æ
        shap.summary_plot(self.shap_values, X, plot_type="bar")
        shap.summary_plot(self.shap_values[1], X)
        
        # è¿”å›é‡è¦å› å­ç´¢å¼•
        return np.argsort(np.abs(self.shap_values[1]).mean(axis=0))[::-1]
    
    def _validate_data(self):
        """éªŒè¯è¾“å…¥æ•°æ®çš„æœ‰æ•ˆæ€§"""
        if not isinstance(self.data, pd.DataFrame):
            raise TypeError("è¾“å…¥æ•°æ®å¿…é¡»æ˜¯pandas DataFrameç±»å‹")
        
        if not isinstance(self.target, pd.Series):
            raise TypeError("ç›®æ ‡å˜é‡å¿…é¡»æ˜¯pandas Seriesç±»å‹")
        
        if len(self.data) != len(self.target):
            raise ValueError("è¾“å…¥æ•°æ®å’Œç›®æ ‡å˜é‡é•¿åº¦ä¸åŒ¹é…")
        
        if self.data.isnull().any().any():
            logger.warning("è¾“å…¥æ•°æ®åŒ…å«ç¼ºå¤±å€¼")
    
    def _log_shap_analysis(self, important_idx):
        """è®°å½•SHAPåˆ†æç»“æœåˆ°wandb"""
        # åˆ›å»ºé‡è¦æ€§è¡¨æ ¼
        importance_table = self.wandb.Table(columns=["Rank", "Factor_Index", "SHAP_Value"])
        
        for rank, idx in enumerate(important_idx):
            importance_table.add_data(rank+1, idx, 
                                     np.abs(self.shap_values[1]).mean(axis=0)[idx])
        
        # åˆ›å»ºSHAPå›¾å¹¶ä¿å­˜
        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
            shap.summary_plot(self.shap_values[1], X, show=False)
            plt.savefig(tmp.name)
            plt.close()
            
            # è®°å½•è¡¨æ ¼å’Œå›¾åƒ
            self.wandb.log({
                "SHAP Importance Ranking": importance_table,
                "SHAP Summary Plot": self.wandb.Image(tmp.name)
            })
            
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            os.unlink(tmp.name)
    
    def run(self):
        """æ‰§è¡Œé—ä¼ ç®—æ³•ä¼˜åŒ–"""
        try:
            pop = self.toolbox.population(n=self.pop_size)
            hof = tools.HallOfFame(self.elite_size)
            stats = tools.Statistics(lambda ind: ind.fitness.values)
            stats.register("avg", np.mean)
            stats.register("min", np.min)
            stats.register("max", np.max)
            
            # ä½¿ç”¨mapå‡½æ•°ç›´æ¥å¤„ç†
            self.toolbox.register("map", map)  # ä½¿ç”¨å†…ç½®mapå‡½æ•°
            
            if self.n_jobs > 1:
                # åˆ›å»ºè¿›ç¨‹æ± 
                with ProcessPoolExecutor(max_workers=self.n_jobs) as executor:
                    def parallel_evaluate(individuals):
                        args_list = [(ind, self.data_array, self.target_array, 
                                    self.base_features, self.use_gpu) for ind in individuals]
                        return list(executor.map(evaluate_individual_wrapper, args_list))
                    
                    # æ›¿æ¢è¯„ä¼°å‡½æ•°
                    original_evaluate = self.toolbox.evaluate
                    self.toolbox.evaluate = lambda ind: evaluate_individual_wrapper((
                        ind, self.data_array, self.target_array, self.base_features, self.use_gpu
                    ))
                    
                    # æ‰§è¡Œè¿›åŒ–
                    best_factors, important_idx, results = self._run_evolution(pop, hof, stats)
                    
                    # æ¢å¤åŸå§‹è¯„ä¼°å‡½æ•°
                    self.toolbox.evaluate = original_evaluate
            else:
                best_factors, important_idx, results = self._run_evolution(pop, hof, stats)
            
            return best_factors, important_idx, results
            
        except Exception as e:
            logger.error(f"é—ä¼ ç®—æ³•è¿è¡Œå¤±è´¥: {str(e)}")
            raise

    def _run_evolution(self, pop, hof, stats):
        """æ‰§è¡Œè¿›åŒ–è¿‡ç¨‹"""
        logger.info("ğŸš€ å¼€å§‹å› å­è¿›åŒ–...")
        start_time = time.time()
        performance_history = []  # æ–°å¢æ€§èƒ½è·Ÿè¸ª
        
        try:
            for gen in range(self.generations):
                gen_start = time.time()
                logger.info(f"\nğŸŒ€ ç¬¬ {gen+1}/{self.generations} ä»£")
                
                pop, log = algorithms.eaSimple(
                    pop, self.toolbox, cxpb=self.cx_prob, mutpb=self.mut_prob,
                    ngen=1, stats=stats, halloffame=hof, verbose=True
                )
                
                # è®°å½•ç²¾è‹±ä¸ªä½“
                logger.info("ğŸ† å½“å‰æœ€ä¼˜å› å­:")
                for idx, ind in enumerate(hof[:3]):  # åªæ˜¾ç¤ºå‰ä¸‰
                    sharpe, ret, win, dd = ind.fitness.values
                    logger.info(
                        f"{idx+1}. {str(ind)[:50]}...\n"
                        f"  å¤æ™®: {sharpe:.2f} | æ”¶ç›Š: {ret:.2%} | "
                        f"èƒœç‡: {win:.2%} | å›æ’¤: {dd:.2%}"
                    )
                
                # æ–°å¢æ€§èƒ½ç›‘æ§
                current_best = hof[0].fitness.values
                performance_history.append({
                    'generation': gen,
                    'sharpe': current_best[0],
                    'return': current_best[1],
                    'max_drawdown': current_best[3],
                    'diversity': len(set(map(str, pop))) / len(pop)  # ç§ç¾¤å¤šæ ·æ€§
                })
                
                # æ—©åœæœºåˆ¶
                if gen > 5 and np.std([p['sharpe'] for p in performance_history[-5:]]) < 0.1:
                    logger.info("ğŸ›‘ å¤æ™®æ¯”ç‡å˜åŒ–å°äº0.1ï¼Œæå‰ç»ˆæ­¢è¿›åŒ–")
                    break
            
            # æœ€ç»ˆæŠ¥å‘Š
            run_time = time.time() - start_time
            logger.success(
                f"\nâœ… è¿›åŒ–å®Œæˆ! è€—æ—¶ {run_time//60:.0f}åˆ†{run_time%60:.0f}ç§’\n"
                f"ğŸ æœ€ä½³å› å­æŒ‡æ ‡:\n"
                f"â–¸ å¤æ™®æ¯”ç‡: {hof[0].fitness.values[0]:.2f}\n"
                f"â–¸ ç´¯è®¡æ”¶ç›Š: {hof[0].fitness.values[1]:.2%}\n"
                f"â–¸ å¹³å‡èƒœç‡: {hof[0].fitness.values[2]:.2%}"
            )
            
            # æ”¶é›†æœ€ä½³å› å­
            best_factors = [ind[0] for ind in hof]
            # æ‰§è¡ŒSHAPåˆ†æ
            important_idx = self._shap_analysis(best_factors)
            
            return best_factors, important_idx, log
            
        except Exception as e:
            logger.critical(f"âš ï¸ è¿›åŒ–ä¸­æ–­! é”™è¯¯: {str(e)}")
            raise

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    try:
        # åŠ è½½é¢„å¤„ç†å¥½çš„é«˜é¢‘æ•°æ®
        logger.info("æ­£åœ¨åŠ è½½æ•°æ®...")
        data = pd.read_feather('train.feather')
        
        # æ£€æŸ¥æ•°æ®åˆ—
        logger.info(f"æ•°æ®åˆ—: {data.columns.tolist()}")
        
        # æ„é€ ç›®æ ‡å˜é‡ï¼šåŸºäºæœªæ¥1åˆ†é’Ÿï¼ˆ120ä¸ª500msï¼‰çš„æ”¶ç›Šç‡
        logger.info("æ„é€ ç›®æ ‡å˜é‡...")
        future_window = 120  # 500ms * 120 = 60ç§’
        future_returns = data['mid_price'].shift(-future_window) / data['mid_price'] - 1
        
        # è®¾ç½®åˆ†ç±»é˜ˆå€¼ï¼ˆä¸‡åˆ†ä¹‹å››=0.0004ï¼‰
        up_threshold = 0.0004
        down_threshold = -0.0004
        
        target = pd.Series(0, index=data.index)  # é»˜è®¤ä¸º0ï¼ˆæŒå¹³ï¼‰
        target[future_returns > up_threshold] = 1    # ä¸Šæ¶¨
        target[future_returns < down_threshold] = -1 # ä¸‹è·Œ
        
        # åˆ é™¤æœ€åfuture_windowè¡Œï¼ˆå› ä¸ºæ— æ³•è®¡ç®—æœªæ¥æ”¶ç›Šï¼‰
        target = target[:-future_window]
        data['return_1min'] = future_returns  # æ–°å¢è¡Œ
        features = data[:-future_window].copy()
        
        # ç§»é™¤ä¸éœ€è¦çš„åˆ—
        features = features.drop(columns=['index', 'mid_price'])
            
        logger.info(f"æ•°æ®å‡†å¤‡å®Œæˆã€‚ç‰¹å¾æ•°é‡: {features.shape[1]}, æ ·æœ¬æ•°é‡: {features.shape[0]}")
        logger.info(f"ç›®æ ‡å˜é‡åˆ†å¸ƒ:\n{target.value_counts(normalize=True)}")
        
        # åˆå§‹åŒ–å› å­æŒ–æ˜å™¨ï¼ˆç§»é™¤wandbç›¸å…³å‚æ•°ï¼‰
        logger.info("åˆå§‹åŒ–å› å­æŒ–æ˜å™¨...")
        miner = GeneticFactorMiner(
            data=features,
            target=target,
            population_size=50,  # å¢å¤§ç§ç¾¤è§„æ¨¡
            generations=20,      # å¢åŠ è¿­ä»£æ¬¡æ•°
            cx_prob=0.5,         # é™ä½äº¤å‰æ¦‚ç‡
            mut_prob=0.4,
            use_gpu=True
        )
        
        # è¿è¡ŒæŒ–æ˜
        logger.info("å¼€å§‹å› å­æŒ–æ˜...")
        best_factors, shap_rank, results = miner.run()
        
        # ä¿å­˜æœ€ä½³å› å­
        output_file = 'best_factors.txt'
        logger.info(f"ä¿å­˜æœ€ä½³å› å­åˆ° {output_file}")
        with open(output_file, 'w') as f:
            f.write(f"ç›®æ ‡å˜é‡æ„é€ æ–¹æ³•ï¼ˆæœªæ¥1åˆ†é’Ÿæ”¶ç›Šç‡ï¼‰ï¼š\n")
            f.write(f"1: æœªæ¥æ”¶ç›Šç‡ > {up_threshold}\n")
            f.write(f"0: {down_threshold} <= æœªæ¥æ”¶ç›Šç‡ <= {up_threshold}\n")
            f.write(f"-1: æœªæ¥æ”¶ç›Šç‡ < {down_threshold}\n\n")
            f.write("æœ€ä½³å› å­ï¼š\n")
            for i, factor in enumerate(best_factors):
                f.write(f"Factor {i+1}: {str(factor)}\n")
        
        logger.info("å› å­æŒ–æ˜å®Œæˆï¼")
        
    except FileNotFoundError:
        logger.error("æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ train.feather")
    except Exception as e:
        logger.error(f"è¿è¡Œå‡ºé”™: {str(e)}")
    finally:
        logger.info("ç¨‹åºè¿è¡Œç»“æŸ") 